{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "在cuda:0上训练\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m train_iter, test_iter \u001b[38;5;241m=\u001b[39m st_train\u001b[38;5;241m.\u001b[39mload_MINST_data(batch_size, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, resize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m)) \u001b[38;5;66;03m# 由于使用的是Fashion-MINST数据集, 进行缩放\u001b[39;00m\n\u001b[1;32m     31\u001b[0m result \u001b[38;5;241m=\u001b[39m st_train\u001b[38;5;241m.\u001b[39mResVisualization((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_acc\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_acc\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m), num_epochs)\n\u001b[0;32m---> 32\u001b[0m st_train\u001b[38;5;241m.\u001b[39mtrain_gpu(AlexNet, train_iter, test_iter, num_epochs, learning_rate, st_train\u001b[38;5;241m.\u001b[39mtry_gpu(), result)\n\u001b[1;32m     33\u001b[0m result\u001b[38;5;241m.\u001b[39mplot_res()\n\u001b[1;32m     34\u001b[0m st_train\u001b[38;5;241m.\u001b[39mstd_prediction_gpu(AlexNet, test_iter, device\u001b[38;5;241m=\u001b[39mst_train\u001b[38;5;241m.\u001b[39mtry_gpu() ,resized\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/Python/DeepLearning/DeepLearning/CNN/st_train.py:157\u001b[0m, in \u001b[0;36mtrain_gpu\u001b[0;34m(net, train_iter, test_iter, num_epochs, learning_rate, device, Res)\u001b[0m\n\u001b[1;32m    155\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 157\u001b[0m             metric\u001b[38;5;241m.\u001b[39madd(loss \u001b[38;5;241m*\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], std_accuracy(y_hat, y), X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    158\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m metric[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m metric[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    159\u001b[0m train_acc \u001b[38;5;241m=\u001b[39m metric[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m metric[\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[0;32m~/Documents/Python/DeepLearning/DeepLearning/CNN/st_train.py:107\u001b[0m, in \u001b[0;36mstd_accuracy\u001b[0;34m(y_hat, y)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# 比较y_hat和y中的每个位置相不相等, 注意这之前要先把它们的类型转换为一样的 .type(dtype)函数表示将这个tensor的类型转为dtype\u001b[39;00m\n\u001b[1;32m    106\u001b[0m cmp \u001b[38;5;241m=\u001b[39m y_hat\u001b[38;5;241m.\u001b[39mtype(y\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m==\u001b[39m y\n\u001b[0;32m--> 107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(cmp\u001b[38;5;241m.\u001b[39mtype(y\u001b[38;5;241m.\u001b[39mdtype)\u001b[38;5;241m.\u001b[39msum())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import st_train\n",
    "from torch import nn\n",
    "# AlexNet是更为复杂的现代卷积神经网络\n",
    "AlexNet = nn.Sequential(\n",
    "# 这里使用一个11*11的更大窗口来捕捉对象。同时，步幅为4，以减少输出的高度和宽度。另外，输出通道的数目远大于LeNet\n",
    "nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
    "nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "# 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数\n",
    "nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
    "nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "# 使用三个连续的卷积层和较小的卷积窗口。\n",
    "# 除了最后的卷积层，输出通道的数量进一步增加。\n",
    "# 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度\n",
    "nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
    "nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
    "nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "nn.Flatten(),\n",
    "# 这里全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合\n",
    "nn.Linear(6400, 4096), nn.ReLU(),\n",
    "nn.Dropout(p=0.5),\n",
    "nn.Linear(4096, 4096), nn.ReLU(),\n",
    "nn.Dropout(p=0.5),\n",
    "# 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000\n",
    "nn.Linear(4096, 10))\n",
    "# 与LeNet相比，这里的主要变化是使用更小的学习速率训练，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵\n",
    "batch_size = 256\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "train_iter, test_iter = st_train.load_MINST_data(batch_size, num_workers=16, resize=(224,224)) # 由于使用的是Fashion-MINST数据集, 进行缩放\n",
    "result = st_train.ResVisualization(('train_acc','test_acc','train_loss'), num_epochs)\n",
    "st_train.train_gpu(AlexNet, train_iter, test_iter, num_epochs, learning_rate, st_train.try_gpu(), result)\n",
    "result.plot_res()\n",
    "st_train.std_prediction_gpu(AlexNet, test_iter, device=st_train.try_gpu() ,resized=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
