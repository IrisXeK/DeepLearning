{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: invalid device function\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 183\u001b[0m\n\u001b[1;32m    181\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Seq2SeqDecoder(\u001b[38;5;28mlen\u001b[39m(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m#net =  utils.EncoderDecoder(encoder, decoder).to(device)\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m seq2seq_train(train_iter, lr, num_epochs, tgt_vocab, device, encoder, decoder)\n\u001b[1;32m    184\u001b[0m engs \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgo .\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi lost .\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhe\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms calm .\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mm home .\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    185\u001b[0m fras \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mva !\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mj\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mai perdu .\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mil est calme .\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mje suis chez moi .\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[11], line 118\u001b[0m, in \u001b[0;36mseq2seq_train\u001b[0;34m(data_iter, lr, num_epochs, target_vocab, device, encoder, decoder)\u001b[0m\n\u001b[1;32m    116\u001b[0m bos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([target_vocab[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<bos>\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m*\u001b[39mY\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], device\u001b[38;5;241m=\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    117\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([bos, Y[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 118\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m encoder(X)\n\u001b[1;32m    119\u001b[0m decoder_state \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39minit_state(encoder_outputs, X_valid_len)\n\u001b[1;32m    120\u001b[0m Y_hat, _ \u001b[38;5;241m=\u001b[39m decoder(decoder_input, decoder_state)\n",
      "File \u001b[0;32m~/pytorch/torch/nn/modules/module.py:1714\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/pytorch/torch/nn/modules/module.py:1725\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1723\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1724\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m, in \u001b[0;36mSeq2SeqEncoder.forward\u001b[0;34m(self, X, *args)\u001b[0m\n\u001b[1;32m     18\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(X) \u001b[38;5;66;03m# 输出shape:(batch_size, num_steps, embed_size)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# 转置后输出shape:(num_steps, batch_size, embed_size) 第一个轴对应时间步\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m output, state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrnn(X)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 输出output的形状为 (num_steps, batch_size, num_hiddens)\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 输出state的形状为  (num_layers, batch_size, num_hiddens)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output, state\n",
      "File \u001b[0;32m~/pytorch/torch/nn/modules/module.py:1714\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1712\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1713\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/pytorch/torch/nn/modules/module.py:1725\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1720\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1723\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1724\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1727\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1728\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/pytorch/torch/nn/modules/rnn.py:1391\u001b[0m, in \u001b[0;36mGRU.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m   1389\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_forward_args(\u001b[38;5;28minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1391\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[1;32m   1392\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1393\u001b[0m         hx,\n\u001b[1;32m   1394\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights,\n\u001b[1;32m   1395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m   1396\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[1;32m   1397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout,\n\u001b[1;32m   1398\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining,\n\u001b[1;32m   1399\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1400\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first,\n\u001b[1;32m   1401\u001b[0m     )\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1403\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mgru(\n\u001b[1;32m   1404\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m   1405\u001b[0m         batch_sizes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[1;32m   1413\u001b[0m     )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: invalid device function\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch, utils, math, collections\n",
    "from torch import nn\n",
    "class Seq2SeqEncoder(utils.Encoder):\n",
    "    \"\"\"\n",
    "    用于序列到序列学习的循环神经网络编码器。\n",
    "    使用了嵌入层(embedding layer)来获得输入序列中每个词元的特征向量。\n",
    "    嵌入层的权重是一个矩阵,其行数等于输入词表的大小(vocab_size),其列数等于特征向量的维度(embed_size)。\n",
    "    对于任意输入词元的索引i,嵌入层获取权重矩阵的第i行(从0开始)以返回其特征向量。\n",
    "    选择了一个多层门控循环单元来实现编码器。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # 嵌入层\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embed_size) \n",
    "        self.rnn = nn.GRU(embed_size, num_hiddens, num_layers, dropout=dropout)\n",
    "\n",
    "    def forward(self, X, *args):\n",
    "        X = self.embedding(X) # 输出shape:(batch_size, num_steps, embed_size)\n",
    "        X = X.permute(1,0,2) # 转置后输出shape:(num_steps, batch_size, embed_size) 第一个轴对应时间步\n",
    "        output, state = self.rnn(X)\n",
    "        # 输出output的形状为 (num_steps, batch_size, num_hiddens)\n",
    "        # 输出state的形状为  (num_layers, batch_size, num_hiddens)\n",
    "        return output, state # 输出和隐状态\n",
    "    \n",
    "# encoder = Seq2SeqEncoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "# encoder.eval()\n",
    "# X = torch.zeros((4, 7), dtype=torch.long)\n",
    "# output, state = encoder(X)\n",
    "# output.shape\n",
    "\n",
    "class Seq2SeqDecoder(utils.Decoder):\n",
    "    \"\"\"\n",
    "    用于序列到序列学习的循环神经网络编码器。\n",
    "    实现解码器时，我们直接使用编码器最后一个时间步的隐状态来初始化解码器的隐状态。\n",
    "    这要求使用循环神经网络实现的编码器和解码器具有相同数量的层和隐藏单元。\n",
    "    为了进一步包含经过编码的输入序列的信息,上下文变量在所有的时间步与解码器的输入进行拼接。\n",
    "    为了预测输出词元的概率分布，在循环神经网络解码器的最后一层使用全连接层来变换隐状态。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens, num_layers, dropout=0, **kwargs):\n",
    "        super(Seq2SeqDecoder, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.rnn = nn.GRU(embed_size + num_hiddens, num_hiddens, num_layers, dropout=dropout)\n",
    "        self.dense = nn.Linear(num_hiddens, vocab_size)\n",
    "\n",
    "    def init_state(self, encoder_outputs, *args):\n",
    "        return encoder_outputs[1] # 使用编码器最后一个时间步的隐状态来初始化解码器的隐状态\n",
    "    \n",
    "    def forward(self, X, state):\n",
    "        X = self.embedding(X).permute(1, 0, 2) # X.shape:(num_steps, batch_size, embed_size)\n",
    "        context = state[-1].repeat(X.shape[0], 1, 1) # 广播context，使其具有与X相同的num_steps\n",
    "        X_and_context = torch.cat((X, context), dim=2) # 在embed_size维度上连接\n",
    "        output, state = self.rnn(X_and_context, state)\n",
    "        output = self.dense(output).permute(1, 0, 2)\n",
    "        # output: (num_steps, batch_size, num_hiddens)\n",
    "        # state:  (num_layers, batch_size, num_hiddens)\n",
    "        return output, state\n",
    "    \n",
    "# decoder = Seq2SeqDecoder(vocab_size=10, embed_size=8, num_hiddens=16, num_layers=2)\n",
    "# decoder.eval()\n",
    "# state = decoder.init_state(encoder(X))\n",
    "# output, state = decoder(X, state)\n",
    "# output.shape, state.shape\n",
    "\n",
    "def sequence_mask(X, valid_len, value=0):\n",
    "    \"\"\"\n",
    "    在序列中屏蔽不相关的项的函数\n",
    "    例如,如果两个序列的有效长度(不包括填充词元)分别为1和2,\n",
    "    则第一个序列的第一项和第二个序列的前两项之后的剩余项将被清除为value指定的值(默认为0)。\n",
    "    即:将填充词元的预测排除在损失函数的计算之外\n",
    "    \"\"\"\n",
    "    maxlen = X.size(1)\n",
    "    mask = torch.arange((maxlen), dtype=torch.float32, device=X.device)[None, :] < valid_len[:, None]\n",
    "    X[~mask] = value\n",
    "    return X\n",
    "\n",
    "class MaskedSoftmaxCrossEntropyLoss(nn.CrossEntropyLoss):\n",
    "    \"\"\"带屏蔽不相关项的交叉熵损失函数\"\"\"\n",
    "    def forward(self, pred, label, valid_len):\n",
    "        \"\"\"\n",
    "        pred的形状:(batch_size,num_steps,vocab_size)\n",
    "        label的形状:(batch_size,num_steps)\n",
    "        valid_len的形状:(batch_size,)\n",
    "        \"\"\"\n",
    "        weights = torch.ones_like(label, device=label.device)\n",
    "        weights = sequence_mask(weights, valid_len)  # 在序列中屏蔽不相关的项\n",
    "        self.reduction ='none'\n",
    "        unweighted_loss = super().forward(pred.permute((0,2,1)),label)\n",
    "        weighted_loss = (unweighted_loss * weights).mean(dim=1)  # 计算加权后的平均损失\n",
    "        return weighted_loss\n",
    "        \n",
    "def seq2seq_train(data_iter, lr, num_epochs, target_vocab, device, encoder, decoder):\n",
    "    \"\"\"训练序列到序列模型\"\"\"\n",
    "    def xavier_init_weights(m):\n",
    "        if type(m) == nn.Linear:\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "        if type(m) == nn.GRU:\n",
    "            for param in m._flat_weights_names:\n",
    "                if \"weight\" in param:\n",
    "                    nn.init.xavier_uniform_(m._parameters[param])\n",
    "    #net.apply(xavier_init_weights)\n",
    "    #net.to(device)\n",
    "    #optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    loss_function = MaskedSoftmaxCrossEntropyLoss()\n",
    "    timer = utils.Timer()\n",
    "    res = utils.ResVisualization(xlist=[[]], ylist=[[]], legend_names=['loss_in_epoch'],\n",
    "                                is_grid=True, title='Result', xlabel='epoch', ylabel='loss')\n",
    "    #net.train()\n",
    "    with timer:\n",
    "        for epoch in range(num_epochs):\n",
    "            metric = utils.Accumulator(2) # 训练损失总和, 词元数量\n",
    "            for batch in data_iter:\n",
    "                #optimizer.zero_grad()\n",
    "                X, X_valid_len, Y, Y_valid_len = [x.to(device) for x in batch]\n",
    "                bos = torch.tensor([target_vocab['<bos>']]*Y.shape[0], device=device).reshape(-1, 1)\n",
    "                decoder_input = torch.cat([bos, Y[:, :-1]], dim=1)\n",
    "                encoder_outputs = encoder(X)\n",
    "                decoder_state = decoder.init_state(encoder_outputs, X_valid_len)\n",
    "                Y_hat, _ = decoder(decoder_input, decoder_state)\n",
    "                #Y_hat, _ = net(X, decoder_input, X_valid_len)\n",
    "                loss = loss_function(Y_hat, Y, Y_valid_len)\n",
    "                loss.sum().backward()\n",
    "                #utils.grad_clipping(net, 1)\n",
    "                num_tokens = Y_valid_len.sum()\n",
    "                #optimizer.step()\n",
    "                with torch.no_grad():\n",
    "                    metric.add(loss.sum(), num_tokens)\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                res.add(epoch+1, metric[0]/metric[1], 'loss_in_epoch')\n",
    "                print(f\"epoch:{epoch+1}, loss:{metric[0]/metric[1]:.3f}\")\n",
    "    print(f\"损失:{metric[0]/metric[1]:.3f}, {metric[1]/timer.get_elapsed_time():.1f} tokens/秒 在 {str(device)}上\")\n",
    "    res.plot_res()\n",
    "\n",
    "def predict_seq2seq(net, src_sentence, src_vocab, tgt_vocab, \n",
    "                    num_steps, device, save_attention_weights=False):\n",
    "    \"\"\"序列到序列模型的预测\"\"\"\n",
    "    net.eval()\n",
    "    src_tokens = src_vocab[src_sentence.lower().split(' ')] + [src_vocab['<eos>']]\n",
    "    encoder_valid_len = torch.tensor([len(src_tokens)], device=device)\n",
    "    src_tokens = utils.truncate_pad(src_tokens, num_steps, src_vocab['<pad>'])\n",
    "    encoder_X = torch.unsqueeze(torch.tensor(src_tokens, dtype=torch.long, device=device), dim=0) # 添加批量轴\n",
    "    encoder_outputs = net.encoder(encoder_X, encoder_valid_len)\n",
    "    decoder_state = net.decoder.init_state(encoder_outputs, encoder_valid_len)\n",
    "    decoder_X = torch.unsqueeze(torch.tensor([tgt_vocab['<bos>']], dtype=torch.long, device=device), dim=0) # 添加批量轴\n",
    "    output_seq, attention_weight_seq = [], []\n",
    "    for _ in range(num_steps):\n",
    "        Y, decoder_state = net.decoder(decoder_X, decoder_state)\n",
    "        decoder_X = Y.argmax(dim=2) # 使用具有最高可能性的词元, 作为解码器在下一时间步的输入\n",
    "        pred = decoder_X.squeeze(dim=0).type(torch.int32).item()\n",
    "        if save_attention_weights: # 保存注意力权重\n",
    "            attention_weight_seq.append(net.decoder.attention.attention_weights)\n",
    "        if pred == tgt_vocab['<eos>']: # 一旦结束词元被预测,输出序列的生成就完成了\n",
    "            break\n",
    "        output_seq.append(pred)\n",
    "    return ''.join(tgt_vocab.to_tokens(output_seq)), attention_weight_seq\n",
    "\n",
    "def bleu(pred_seq, label_seq, k): #@save\n",
    "    \"\"\"计算BLEU\"\"\"\n",
    "    pred_tokens, label_tokens = pred_seq.split(' '), label_seq.split(' ')\n",
    "    len_pred, len_label = len(pred_tokens), len(label_tokens)\n",
    "    score = math.exp(min(0, 1 - len_label / len_pred))\n",
    "    for n in range(1, k + 1):\n",
    "        num_matches, label_subs = 0, collections.defaultdict(int)\n",
    "        for i in range(len_label - n + 1):\n",
    "            label_subs[' '.join(label_tokens[i: i + n])] += 1\n",
    "        for i in range(len_pred - n + 1):\n",
    "            if label_subs[' '.join(pred_tokens[i: i + n])] > 0:\n",
    "                num_matches += 1\n",
    "                label_subs[' '.join(pred_tokens[i: i + n])] -= 1\n",
    "        score *= math.pow(num_matches / (len_pred - n + 1), math.pow(0.5, n))\n",
    "    return score\n",
    "\n",
    "\n",
    "embed_size, num_hiddens, num_layers, dropout = 32, 32, 2, 0.1\n",
    "batch_size, num_steps = 64, 10\n",
    "lr, num_epochs, device = 0.005, 300, utils.try_gpu()\n",
    "train_iter, src_vocab, tgt_vocab =  utils.load_data_nmt(batch_size, num_steps)\n",
    "\n",
    "encoder = Seq2SeqEncoder(len(src_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "decoder = Seq2SeqDecoder(len(tgt_vocab), embed_size, num_hiddens, num_layers, dropout)\n",
    "#net =  utils.EncoderDecoder(encoder, decoder).to(device)\n",
    "seq2seq_train(train_iter, lr, num_epochs, tgt_vocab, device, encoder, decoder)\n",
    "engs = ['go .', \"i lost .\", 'he\\'s calm .', 'i\\'m home .']\n",
    "fras = ['va !', 'j\\'ai perdu .', 'il est calme .', 'je suis chez moi .']\n",
    "# for eng, fra in zip(engs, fras):\n",
    "#     translation, attention_weight_seq = predict_seq2seq(net, eng, src_vocab, tgt_vocab, num_steps, device)\n",
    "#print(f'{eng} => {translation}, bleu {bleu(translation, fra, k=2):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{X}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
