{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.2106, -0.3097, -0.0554,  0.1624,  0.3370,  0.0260,  0.0954, -0.0193]])), ('bias', tensor([0.0957]))])\n",
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([0.0957], requires_grad=True)\n",
      "tensor([0.0957])\n",
      "('weight', torch.Size([8, 4])) ('bias', torch.Size([8]))\n",
      "('0.weight', torch.Size([8, 4])) ('0.bias', torch.Size([8])) ('2.weight', torch.Size([1, 8])) ('2.bias', torch.Size([1]))\n",
      "rgnet(X): tensor([[0.0078],\n",
      "        [0.0078]], grad_fn=<AddmmBackward0>)\n",
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block 0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block 3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n",
      "第一个块的第二个子块的第一层的偏置:tensor([-0.2664, -0.1642, -0.2753,  0.3748, -0.1198, -0.3723, -0.2783,  0.1956])\n",
      "init_normal后tensor([-0.0082,  0.0142, -0.0077,  0.0176]), 0.0\n",
      "init_const后tensor([1., 1., 1., 1.]), 0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# 一、参数访问\n",
    "\"\"\"\n",
    "当通过Sequential类定义模型时,可以通过索引来访问模型的任意层。\n",
    "这就像模型是一个列表一样,每层的参数都在其属性中\n",
    "\"\"\"\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4,8),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(8,1))\n",
    "print(net[2].state_dict())\n",
    "# 每个参数都表示为参数类的一个实例。要对参数执行任何操作,首先需要访问底层的数值\n",
    "print(type(net[2].bias)) # print出的class 'torch.nn.parameter.Parameter'就是一个参数类\n",
    "print(net[2].bias) # net[2].bias 就是一个参数类实例 参数是复合的对象,包含值、梯度和额外信息\n",
    "print(net[2].bias.data) # 通过参数类实例访问参数值\n",
    "# 一次性访问所有参数\n",
    "print(*[(name, param.shape) for name, param in net[0].named_parameters()]) # 访问第一个全连接层的参数\n",
    "print(*[(name, param.shape) for name, param in net.named_parameters()]) # 访问所有层的参数\n",
    "net.state_dict()['2.bias'].data # 通过state_dict传入 层号.参数名 访问网络参数\n",
    "\n",
    "def block1():\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(4,8),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(8,4),\n",
    "        nn.ReLU())\n",
    "\n",
    "def block2():\n",
    "    net = nn.Sequential()\n",
    "    for i in range(4): # 此处进行块的嵌套\n",
    "        net.add_module(f'block {i}',block1())\n",
    "    return net\n",
    "rgnet = nn.Sequential(\n",
    "    block2(), \n",
    "    nn.Linear(4,1))\n",
    "X = torch.rand(size=(2, 4))\n",
    "print(f\"rgnet(X): {rgnet(X)}\")\n",
    "print(rgnet) # 打印rgnet的结构\n",
    "print(f\"第一个块的第二个子块的第一层的偏置:{rgnet[0][1][0].bias.data}\") # 由于层是分层嵌套的,也可以向通过嵌套列表索引一样访问\n",
    "\n",
    "# 二、参数初始化\n",
    "\"\"\"\n",
    "默认情况下,PyTorch会根据一个范围均匀地初始化权重和偏置矩阵,这个范围是根据输入和输出维度计算出的。\n",
    "PyTorch的nn.init模块提供了多种预置初始化方法。\n",
    "\"\"\"\n",
    "def init_normal(m): # 调用pytorch的内置初始化\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.normal_(m.weight, mean=0, std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "def init_const(m): # 将参数初始化为给定的常数\n",
    "    if type(m) == nn.Linear:\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)\n",
    "print(f\"init_normal后{net[0].weight.data[0]}, {net[0].bias.data[0]}\")\n",
    "net.apply(init_const)\n",
    "print(f\"init_const后{net[0].weight.data[0]}, {net[0].bias.data[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
