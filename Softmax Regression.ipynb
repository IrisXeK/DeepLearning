{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 183\u001b[0m\n\u001b[0;32m    181\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(num_outputs, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# 初始化偏置矩阵\u001b[39;00m\n\u001b[0;32m    182\u001b[0m train_set, test_set \u001b[38;5;241m=\u001b[39m load_data(batch_size)\n\u001b[1;32m--> 183\u001b[0m train(net,train_set,test_set,cross_entropy,num_epochs,updater)\n\u001b[0;32m    184\u001b[0m prediction(net, test_set)\n",
      "Cell \u001b[1;32mIn[1], line 149\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net, train_set, test_set, loss_function, num_epochs, updater)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(net, train_set, test_set, loss_function, num_epochs, updater): \u001b[38;5;66;03m# 训练模型\u001b[39;00m\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;66;03m#animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\u001b[39;00m\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;66;03m#                    legend=['train loss', 'train acc', 'test acc'])\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m--> 149\u001b[0m         train_metrics \u001b[38;5;241m=\u001b[39m train_epoch(net, train_set, loss_function, updater)\n\u001b[0;32m    150\u001b[0m         \u001b[38;5;28mprint\u001b[39m(train_metrics)\n\u001b[0;32m    151\u001b[0m         test_accurancy \u001b[38;5;241m=\u001b[39m evaluate_accuracy(net, test_set)\n",
      "Cell \u001b[1;32mIn[1], line 133\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(net, train_set, loss_function, updater)\u001b[0m\n\u001b[0;32m    131\u001b[0m     net\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    132\u001b[0m metric \u001b[38;5;241m=\u001b[39m Accumulator(\u001b[38;5;241m3\u001b[39m) \u001b[38;5;66;03m# 三个位置为 训练损失总和 训练准确数 样本数\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X,y \u001b[38;5;129;01min\u001b[39;00m train_set:\n\u001b[0;32m    134\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m net(X, W) \u001b[38;5;66;03m# 给出一次预测\u001b[39;00m\n\u001b[0;32m    135\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(y_hat, y) \u001b[38;5;66;03m# 计算损失\u001b[39;00m\n",
      "File \u001b[1;32mh:\\Program Files\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mh:\\Program Files\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data()\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mh:\\Program Files\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_get_data()\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mh:\\Program Files\\Anaconda3\\envs\\pytorch\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[0;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[0;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_queue\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[0;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[0;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[1;32mh:\\Program Files\\Anaconda3\\envs\\pytorch\\Lib\\multiprocessing\\queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[1;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[1;32mh:\\Program Files\\Anaconda3\\envs\\pytorch\\Lib\\multiprocessing\\connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[1;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout)\n",
      "File \u001b[1;32mh:\\Program Files\\Anaconda3\\envs\\pytorch\\Lib\\multiprocessing\\connection.py:346\u001b[0m, in \u001b[0;36mPipeConnection._poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_got_empty_message \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m    344\u001b[0m             _winapi\u001b[38;5;241m.\u001b[39mPeekNamedPipe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(wait([\u001b[38;5;28mself\u001b[39m], timeout))\n",
      "File \u001b[1;32mh:\\Program Files\\Anaconda3\\envs\\pytorch\\Lib\\multiprocessing\\connection.py:1083\u001b[0m, in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m   1080\u001b[0m                 ready_objects\u001b[38;5;241m.\u001b[39madd(o)\n\u001b[0;32m   1081\u001b[0m                 timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1083\u001b[0m     ready_handles \u001b[38;5;241m=\u001b[39m _exhaustive_wait(waithandle_to_obj\u001b[38;5;241m.\u001b[39mkeys(), timeout)\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;66;03m# request that overlapped reads stop\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ov \u001b[38;5;129;01min\u001b[39;00m ov_list:\n",
      "File \u001b[1;32mh:\\Program Files\\Anaconda3\\envs\\pytorch\\Lib\\multiprocessing\\connection.py:1015\u001b[0m, in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m   1013\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m L:\n\u001b[1;32m-> 1015\u001b[0m     res \u001b[38;5;241m=\u001b[39m _winapi\u001b[38;5;241m.\u001b[39mWaitForMultipleObjects(L, \u001b[38;5;28;01mFalse\u001b[39;00m, timeout)\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;241m==\u001b[39m WAIT_TIMEOUT:\n\u001b[0;32m   1017\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import torch \n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "class Accumulator: # 累加多个变量的实用程序类\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0]*n\n",
    "    def add(self,*args) : # 在data的对应位置加上对应的数\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "    def reset(self):\n",
    "        self.data=[0.0] * len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "class Animator: # 绘制数据的实用程序类\n",
    "    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
    "        ylim=None, xscale='linear', yscale='linear',\n",
    "        fmts=('-', 'm--', 'g-.', 'r:'), n_rows=1, n_cols=1,\n",
    "        figsize=(3.5, 2.5)): \n",
    "        # xlim ylim指定x轴和y轴的范围\n",
    "        # 增量地绘制多条线\n",
    "        if legend is None:\n",
    "            legend = []\n",
    "        self.fig, self.axes = plt.subplots(n_rows, n_cols, figsize=figsize) # 这里fig存储的是subplots画出的整个图 axes是一个二维索引数组,储存每个子图\n",
    "        if n_rows * n_cols == 1:\n",
    "            self.axes = [self.axes, ]\n",
    "        # 使用lambda函数捕获参数\n",
    "        self.config_axes = lambda: self.axes[0].set(\n",
    "        xlabel=xlabel, ylabel=ylabel, xlim=xlim, ylim=ylim, xscale= xscale, yscale =yscale)\n",
    "        self.X, self.Y, self.fmts = None, None, fmts\n",
    "    def add(self, x, y):\n",
    "        # 向图表中添加多个数据点\n",
    "        if not hasattr(y, \"__len__\"): # 如果y是一个单一的单位 将它变成一个可迭代的列表 这样使函数可以处理单个数据点\n",
    "            y = [y]\n",
    "        n = len(y)\n",
    "        if not hasattr(x, \"__len__\"): # 如果x是一个单一的单位 将它变成一个和y同规模的可迭代的列表\n",
    "            x = [x] * n\n",
    "        if not self.X:\n",
    "            self.X = [[] for _ in range(n)]\n",
    "        if not self.Y:\n",
    "            self.Y = [[] for _ in range(n)]\n",
    "        for i, (a, b) in enumerate(zip(x, y)):\n",
    "            if a is not None and b is not None:\n",
    "                self.X[i].append(a)\n",
    "                self.Y[i].append(b)\n",
    "        self.axes[0].cla()\n",
    "        for x, y, fmt in zip(self.X, self.Y, self.fmts):\n",
    "            self.axes[0].plot(x, y, fmt)\n",
    "        self.config_axes()\n",
    "        display(self.fig)\n",
    "        #display(clear=True)\n",
    "#小批量的Softmax回归\n",
    "# 使用Fashion-MINST数据集 集中的图片大小为28*28\n",
    "def get_MINST_labels(labels): # 获取训练集中的数据对应的标签 labels参数传入 MINST_train.train_labels\n",
    "    text_labels=['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "    'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "    return [text_labels[int(i)] for i in labels]\n",
    "def load_data(batch_size, resize = False):\n",
    "    trans =[ transforms.ToTensor()] # 格式转换的列表 列表里的所有内容在格式转换时都会被应用\n",
    "    if resize: # 如果指定了需要重置大小\n",
    "        trans.insert(0,transforms.Resize(resize)) #在trans列表开头位置添加transforms.Resize类\n",
    "    trans = transforms.Compose(trans) # 将trans列表中的内容整合在一起 形成一个整合类 之后可调用 transformedd_img=trans(img) 初始化img\n",
    "    MINST_train = torchvision.datasets.FashionMNIST(root='./data', transform=trans, train=True, download=True)\n",
    "    MINST_test = torchvision.datasets.FashionMNIST(root='./data', transform=trans, train=False, download=True)\n",
    "    return (data.DataLoader(MINST_train, batch_size, shuffle=True, num_workers=16),\n",
    "            data.DataLoader(MINST_test, batch_size, shuffle=False, num_workers=16))\n",
    "def show_imgs(imgs, num_rows, num_cols, titles=None, scale=1.5):# 将所有图片绘制在一张表中\n",
    "    \"\"\"\n",
    "    参数: imgs为图片集 num_rows*num_cols = 绘制的图片总数 即将所有图片展示为num_rows行num_cols列\n",
    "    title为绘制出的图片标题 scale为缩放大小 如1.5为放大1.5倍绘制\n",
    "    \"\"\"\n",
    "    figure_size = (num_cols*scale, num_rows*scale)\n",
    "    _, axes = plt.subplots(num_rows,num_cols,figsize=figure_size)\n",
    "    axes = axes.flatten()\n",
    "    for i, (ax,img) in enumerate(zip(axes,imgs)):\n",
    "        if(torch.is_tensor(img)): #如果图片用张量存储\n",
    "            ax.imshow(img.numpy())\n",
    "        else: #PIL图片\n",
    "            ax.imshow(img)\n",
    "        ax.axes.get_xaxis().set_visible(False) # 取消图中的X轴上的坐标刻度和坐标数值\n",
    "        ax.axes.get_yaxis().set_visible(False) # 取消图中的Y轴上的坐标刻度和坐标数值\n",
    "        if(titles): # 如果绘制时指定了标题 就显示标题 否则不显示\n",
    "            ax.set_title(titles[i])\n",
    "    return axes\n",
    "def Softmax(X): # Softmax基于随机输入,将每个元素变成非负数,依据概率原理,每行的总和为1\n",
    "    \"\"\"\n",
    "    Softmax(X)[i][j] = exp(X[i][j]) / Sigma(X[i][k]) 即每个位置的Softmax就是这个位置的值的Exp除以这一行的Exp求和\n",
    "    \"\"\"\n",
    "    X_exp = torch.exp(X) #Softmax的分子\n",
    "    partition = X_exp.sum(1, keepdim=True) #Softmax的分母\n",
    "    return X_exp / partition # 使用了广播机制\n",
    "def net(X, W): # 定义模型\n",
    "    return Softmax(torch.matmul(X.reshape(-1,W.shape[0]),W) + b)\n",
    "def cross_entropy(y_hat, y):\n",
    "    \"\"\"\n",
    "    交叉熵采用真实标签的预测概率的负对数似然\n",
    "    y[]是一个张量,表示每个数据的正确预测的标签\n",
    "    y_hat[range(len(y_hat)), y] 表达式表示对于每个y[i],找到对应的y_hat预测的概率\n",
    "    比如：\n",
    "    y = torch.tensor([0, 2])\n",
    "    y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])\n",
    "    y_hat[[0, 1], y] 输出tensor([0.1000, 0.5000])\n",
    "    表示 因为y中正确的标签是 0类 和 2类\n",
    "    取出 y_hat中每行的y_hat[i][y[correct_label]]的概率\n",
    "    \"\"\"\n",
    "    return -torch.log(y_hat[range(len(y_hat)), y])\n",
    "def accuracy(y_hat, y): # 计算预测正确的数量\n",
    "    \"\"\"\n",
    "    如果y_hat存储的是矩阵,假定第二个维度存储每个类的预测分数\n",
    "    \"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis = 1) #取出y_hat中每一行中的最大的那个概率的索引\n",
    "    cmp = y_hat.type(y.dtype) == y # 比较y_hat和y中的每个位置相不相等, 注意这之前要先把它们的类型转换为一样的 .type(dtype)函数表示将这个tensor的类型转为dtype\n",
    "    return float(cmp.type(y.dtype).sum())\n",
    "def evaluate_accuracy(net,data_iter): # 对于任何data_iter可访问的数据集 都可以评估模型的精度\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.eval() # 模型设置为评估模式\n",
    "    metric = Accumulator(2) # 2个位置为 正确预测数和预测总数\n",
    "    with torch.no_grad():\n",
    "        for X, y in data_iter:\n",
    "            metric.add(accuracy(net(X,W),y),y.numel())\n",
    "    return metric[0] / metric[1]\n",
    "def train_epoch(net, train_set, loss_function, updater): # 模型在训练周期中的一次训练\n",
    "    \"\"\"\n",
    "    updater是更新模型参数的函数,接收批量大小作为参数\n",
    "    updater可以是sgd函数 也可以是框架内的内置函数\n",
    "    \"\"\"\n",
    "    if isinstance(net, nn.Module):\n",
    "        net.train()\n",
    "    metric = Accumulator(3) # 三个位置为 训练损失总和 训练准确数 样本数\n",
    "    for X,y in train_set:\n",
    "        y_hat = net(X, W) # 给出一次预测\n",
    "        loss = loss_function(y_hat, y) # 计算损失\n",
    "        if isinstance(updater, torch.optim.Optimizer):# updater为Pytorch框架的内置优化器\n",
    "            updater.zero_grad() # 将grad置为0 因为pytorch计算梯度时会累加\n",
    "            loss.mean().backward() # 计算梯度\n",
    "            updater.step() # 由计算出的梯度更新参数\n",
    "        else: # 使用的是定制的优化器和损失函数\n",
    "            loss.sum().backward()\n",
    "            updater(X.shape[0])\n",
    "    metric.add(float(loss.sum()), accuracy(y_hat, y), y.numel())\n",
    "    return metric[0] / metric[2], metric[1] / metric[2] # 返回训练损失和训练精度\n",
    "def train(net, train_set, test_set, loss_function, num_epochs, updater): # 训练模型\n",
    "    #animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],\n",
    "    #                    legend=['train loss', 'train acc', 'test acc'])\n",
    "    for epoch in range(num_epochs):\n",
    "        train_metrics = train_epoch(net, train_set, loss_function, updater)\n",
    "        print(train_metrics)\n",
    "        test_accurancy = evaluate_accuracy(net, test_set)\n",
    "        #animator.add(epoch + 1, train_metrics + (test_accurancy,))\n",
    "    train_loss, train_accuracy = train_metrics\n",
    "    #assert train_loss < 0.5, train_loss\n",
    "    #assert train_accuracy <= 1 and train_accuracy > 0.7, train_accuracy\n",
    "    #assert test_accurancy <= 1 and test_accurancy > 0.7, test_accurancy\n",
    "def sgd(params,lr,batch_size):\n",
    "    with torch.no_grad():\n",
    "        for param in params:\n",
    "            param-= learning_rate * param.grad / batch_size\n",
    "            param.grad.zero_()\n",
    "def updater(batch_size):\n",
    "    return sgd(params=[W,b], lr = learning_rate, batch_size=batch_size)\n",
    "def prediction(net, test_set,n=6): \n",
    "    for X ,y in test_set:\n",
    "        _, axes = plt.subplots(1,n,figsize= (8,8))\n",
    "        true_labels = get_MINST_labels(y)\n",
    "        pred_labels= get_MINST_labels(net(X,W).argmax(axis = 1))\n",
    "        titles = [true + '\\n' + pred for true , pred in zip(true_labels,pred_labels)]\n",
    "        for i in range(n):\n",
    "            axes[i].imshow(X[i].reshape((28,28)))\n",
    "            axes[i].set_title(titles[i])\n",
    "            axes[i].axis('off')\n",
    "# 预测图片类别\n",
    "batch_size = 256\n",
    "num_inputs = 784 # 一个图片有784个像素 即输入维度有784\n",
    "num_outputs = 10 # 共有10个类别 即输出维度为10\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10\n",
    "W = torch.normal(0, 0.01, size=(num_inputs,num_outputs), requires_grad=True) # 初始化权重矩阵\n",
    "b = torch.zeros(num_outputs, requires_grad=True) # 初始化偏置矩阵\n",
    "train_set, test_set = load_data(batch_size)\n",
    "train(net,train_set,test_set,cross_entropy,num_epochs,updater)\n",
    "prediction(net, test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
